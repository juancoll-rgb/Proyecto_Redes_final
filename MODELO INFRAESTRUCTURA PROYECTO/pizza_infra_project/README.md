
# ğŸ• Proyecto Final â€” *Infraestructura AnalÃ­tica Distribuida: Pizza Sales*

## ğŸ“Œ DescripciÃ³n General

Este proyecto implementa un **pipeline analÃ­tico distribuido de extremo a extremo**, capaz de procesar un dataset de ventas de pizzas mediante **Apache Spark**, exponer resultados agregados mediante **APIs Flask balanceadas con Nginx**, y visualizar mÃ©tricas clave en un **dashboard interactivo con Streamlit**.

La arquitectura sigue el paradigma de **microservicios contenedorizados**, orquestados mediante Docker Compose, lo que garantiza portabilidad, escalabilidad y replicabilidad.

---

## âš™ï¸ Stack TecnolÃ³gico

| Componente | Rol | TecnologÃ­a |
|-------------|-----|------------|
| **ETL distribuido** | Limpieza, transformaciÃ³n y agregaciÃ³n de datos | ğŸ§  Apache Spark (1 master + 2 workers) |
| **Backend REST** | ExposiciÃ³n de KPIs procesados | âš™ï¸ Flask (2 rÃ©plicas) |
| **Balanceo de carga** | DistribuciÃ³n de trÃ¡fico a rÃ©plicas API | ğŸŒ Nginx |
| **Dashboard** | VisualizaciÃ³n interactiva de resultados | ğŸ“Š Streamlit |
| **OrquestaciÃ³n** | CoordinaciÃ³n de servicios | ğŸ³ Docker Compose |

---

## ğŸ§± Estructura del Proyecto

```
pizza_infra_project/
â”‚
â”œâ”€â”€ api/                # API Flask (REST) para exponer KPIs
â”œâ”€â”€ dashboard/          # AplicaciÃ³n Streamlit (visualizaciÃ³n)
â”œâ”€â”€ data/               # Dataset fuente (pizza_sales.csv)
â”œâ”€â”€ nginx/              # ConfiguraciÃ³n del balanceador Nginx
â”œâ”€â”€ out/                # Salidas del ETL (JSON generados por Spark)
â”œâ”€â”€ spark/              # Script ETL PySpark + Dockerfile
â””â”€â”€ docker-compose.yml  # OrquestaciÃ³n de todos los servicios
```

---

## ğŸªœ Paso a paso para reproducir la implementaciÃ³n

### 1ï¸âƒ£ Requisitos previos

- Instalar **Docker Desktop** (Windows/Mac) o **Docker Engine + Compose** (Linux)
- Tener al menos **4 GB de RAM** disponibles
- Clonar o descargar el repositorio completo del proyecto

```bash
git clone https://github.com/usuario/pizza_infra_project.git
cd pizza_infra_project
```

---

### 2ï¸âƒ£ Verificar la estructura y dataset

AsegÃºrate de tener el archivo `pizza_sales.csv` en la carpeta `/data`.  
Puedes usar un dataset de ejemplo con las columnas:
```
order_details_id,order_id,pizza_id,quantity,order_date,order_time,
unit_price,total_price,pizza_size,pizza_category,pizza_ingredients,pizza_name
```

---

### 3ï¸âƒ£ Construir y desplegar la infraestructura

Ejecuta en PowerShell o Terminal:

```bash
docker compose up -d --build
```

Esto levanta los siguientes servicios:
- Spark master y 2 workers
- Flask API (2 rÃ©plicas)
- Nginx como balanceador de carga
- Streamlit Dashboard

---

### 4ï¸âƒ£ Confirmar servicios en ejecuciÃ³n

Verifica que los contenedores estÃ©n activos:

```bash
docker ps
```

Debes ver algo similar a:
```
spark-master
spark-worker-1
spark-worker-2
spark-submit
api1
api2
nginx-lb
dashboard
```

---

### 5ï¸âƒ£ Consultar interfaces

| Servicio | DescripciÃ³n | URL / Puerto |
|-----------|-------------|--------------|
| **Spark Master UI** | Supervisar el ETL | [http://localhost:8080](http://localhost:8080) |
| **Dashboard Streamlit** | VisualizaciÃ³n de resultados | [http://localhost:8501](http://localhost:8501) |
| **API Balanceada (Nginx)** | Endpoints REST | [http://localhost/api/...](http://localhost/api/...) |

---

### 6ï¸âƒ£ Validar ejecuciÃ³n del ETL

El contenedor `spark-submit` ejecuta automÃ¡ticamente el script `/spark/pizza_sales_etl.py`, que genera archivos `.json` en `/data/out`.  
Puedes verificar los resultados con:

```bash
docker logs spark-submit -f
```

Y luego copiar los resultados al host:
```bash
docker cp spark-submit:/data/out ./out_check
```

---

### 7ï¸âƒ£ Consultar los endpoints REST

Ejemplo de pruebas:

```bash
curl http://localhost/api/categories
curl http://localhost/api/top_pizzas
curl http://localhost/api/daily_kpis
```

---

### 8ï¸âƒ£ Visualizar resultados en el dashboard

Abre tu navegador en:  
ğŸ‘‰ [http://localhost:8501](http://localhost:8501)

VerÃ¡s las grÃ¡ficas con ventas por categorÃ­a, tamaÃ±o, top 5 pizzas y ticket promedio diario.

---

### 9ï¸âƒ£ Limpieza del entorno

Para detener y eliminar todos los servicios, redes y volÃºmenes:

```bash
docker compose down -v
```

---

## ğŸ§­ Arquitectura General

```
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ pizza_sales.csvâ”‚
                   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Spark Cluster (ETL) â”‚
                â”‚  1 master + 2 workersâ”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ JSON KPIs
                          â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Flask API x2        â”‚
                â”‚ (REST microservices)â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
                    â”‚ Nginx LB  â”‚
                    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                          â”‚
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚ Streamlit Dashboard     â”‚
             â”‚  VisualizaciÃ³n de KPIs  â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’¡ Notas finales

- Si realizas cambios en el script ETL (`pizza_sales_etl.py`), vuelve a ejecutar:
  ```bash
  docker compose up -d --build
  ```
- Puedes ver los logs en tiempo real:
  - `docker logs spark-submit -f` (ETL)
  - `docker logs api1 -f` (API Flask)
- El dashboard se actualiza automÃ¡ticamente al regenerar los archivos JSON en `/out`.

---
